{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network - Dog vs. Cat Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will train a Convolutional Neural Network to identify dogs and cats.\n",
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "# Sequential is what we will use to initialize our Neural Network\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Conv2D is what we use to add our Convolutional layers.\n",
    "# 2D for images. 3D would be for video (adding time)\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "# MaxPooling2D is what we will use for the pooling step\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "# Flatten allows us to turn our pooling layer into a large feature input vector\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# Dense to add fully connected layers\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution](convolution.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call Conv2D to create a 2-dimensional CNN\n",
    "# 32 is the number of feature detectors in our image of 3x3 dimensions\n",
    "# Our Convolution Layer is composed of 32 feature maps\n",
    "# Input shape is the shape of our input images. 64x64 pixels & 3 channels because we are using color images\n",
    "# use rectifier function for activation\n",
    "classifier.add(Conv2D(32, (3,3), input_shape = (64,64,3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![input_shape](input_shape.png)\n",
    "![dog_rgb](dog_rgb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "Pooling condenses the feature map (generated by the convolution) into a pooled feature map. It does this by cycling through the feature map with a 2x2 square and stores the max number of features found in each cycle. The main reason we do this is to reduce the number of nodes in the Flattening step. Moreover, this reduces complexity and time execution without losing performance.\n",
    "\n",
    "![maxpool](maxpool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set our pool_size to 2x2, so we cycle through conv. with a 2x2 box\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Second Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note we do not need to provide the input shape because it's coming from 1st conv. layer\n",
    "# Same filter, kernel size, & activation function\n",
    "classifier.add(Conv2D(32, (3,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening\n",
    "The flattening process converts the pooling layer into a future input layer for an artificial neural network. We take all of our pooled feature maps and put them into one single vector. \n",
    "\n",
    "![flattening](flattening.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Important Flattening Questions\n",
    "\n",
    " 1. *Why don't we lose the spacial structure by flattening all these feature maps into one single vector?*\n",
    " \n",
    " By creating our feature maps, we extracted the spacial structure information by representing spacial structure with larger numbers. These large numbers were generated thanks to the feature detectors that we applied on the input image in the *Convolution* step. High numbers in the feature maps are associated to a specific feature in the input image. Then by applying *Max Pooling* we keep the large numbers because we take the max. The flattening step just consists of putting the numbers from the cells in the pooling layer into one single vector.<br>\n",
    " <br>\n",
    " 2. *Why didn't we just skip the Convolution and Pooling steps and, instead, take all the pixels from the input image and flatten them into the one single vector?*\n",
    " \n",
    " If we were to flatten the pixels from the input image and put them into one single vector, then each node of this massive vector would represent one pixel of the image independent from its surrounding pixels. We would only gain information from a single pixel instead of how each pixel is spacially connected to pixels around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Connection\n",
    "We will use our flattened vector as the input to a classic artificial neural network, because an ANN is a great classifier for non-linear problems, such as image classification. The Dense() function creates a new layer, known as a fully connected layer, where every input node is connected to every fully connected layer node.\n",
    "\n",
    "![full_connection](full_connection.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take our classifier and add a fully-connected hidden layer\n",
    "# A common practice that leads to good results is to set the # of input nodes to around 100\n",
    "# 128 because it is a power of 2\n",
    "# Set activation function to rectifier function\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# Now add our output layer\n",
    "# It will only output one node - dog or a cat\n",
    "# Because our output is binomial, we choose the sigmoid function\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the CNN\n",
    "\n",
    "Last thing remaining is to compile our CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call compile and set optimizer to 'adam' for gradient descent\n",
    "# set our loss function to binary_crossentropy because it corresponds to logarithmic loss & binary outcome\n",
    "# if we had more than 2 outcomes, we would just choose 'crossentropy'\n",
    "# Use accuracy as our performance metric\n",
    "classifier.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log_loss](log_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN to the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Keras module\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce overfitting, we will implement an image augmentation trick that allows us to enrich our training set without needing to add more images. It does this by randomly tweaking some of the images by zooming in, blurring, rotating, etc. This will allow us to generate good performance results while minimizing the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rescale multiplies the image data by the number provided\n",
    "# shear range determines the shear intensity (in radians)\n",
    "# zoom range determines the amount of zoom\n",
    "# set horizontal_flip to True so model will randomly flip images horizontally\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same for test data\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a training_set composed of all augmented images extracted from our image data generator.\n",
    "# Set path to image directory\n",
    "# Set target_size to 64x64 pixels because these dimensions are expected by CNN\n",
    "# 32 is the size of our batches that include random samples\n",
    "# batch_size also sets the number of images to pass through the CNN before the weights are updated\n",
    "# Class_mode = binary because we have 2 classes - cats and dogs\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set', target_size = (64,64), batch_size = 32, class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a test_set composed of all augmented images extracted from our image data generator.\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set', target_size = (64,64), batch_size = 32, class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 5775s - loss: 0.3653 - acc: 0.8243 - val_loss: 0.5784 - val_acc: 0.8041\n",
      "Epoch 2/20\n",
      "8000/8000 [==============================] - 5049s - loss: 0.1093 - acc: 0.9592 - val_loss: 0.8479 - val_acc: 0.8033\n",
      "Epoch 3/20\n",
      "8000/8000 [==============================] - 6640s - loss: 0.0601 - acc: 0.9787 - val_loss: 0.9198 - val_acc: 0.8065\n",
      "Epoch 4/20\n",
      "8000/8000 [==============================] - 5869s - loss: 0.0462 - acc: 0.9841 - val_loss: 1.0391 - val_acc: 0.7902\n",
      "Epoch 5/20\n",
      "8000/8000 [==============================] - 6030s - loss: 0.0367 - acc: 0.9878 - val_loss: 1.1386 - val_acc: 0.8066\n",
      "Epoch 6/20\n",
      "8000/8000 [==============================] - 34494s - loss: 0.0303 - acc: 0.9898 - val_loss: 1.1780 - val_acc: 0.8000\n",
      "Epoch 7/20\n",
      "8000/8000 [==============================] - 6087s - loss: 0.0252 - acc: 0.9917 - val_loss: 1.2654 - val_acc: 0.7971\n",
      "Epoch 8/20\n",
      "8000/8000 [==============================] - 5221s - loss: 0.0238 - acc: 0.9924 - val_loss: 1.2042 - val_acc: 0.8033\n",
      "Epoch 9/20\n",
      "8000/8000 [==============================] - 9015s - loss: 0.0208 - acc: 0.9933 - val_loss: 1.2575 - val_acc: 0.8051\n",
      "Epoch 10/20\n",
      "8000/8000 [==============================] - 8636s - loss: 0.0188 - acc: 0.9940 - val_loss: 1.3217 - val_acc: 0.8038\n",
      "Epoch 11/20\n",
      "8000/8000 [==============================] - 5964s - loss: 0.0187 - acc: 0.9940 - val_loss: 1.2774 - val_acc: 0.8237\n",
      "Epoch 12/20\n",
      "8000/8000 [==============================] - 4521s - loss: 0.0171 - acc: 0.9946 - val_loss: 1.3890 - val_acc: 0.8059\n",
      "Epoch 13/20\n",
      "8000/8000 [==============================] - 6033s - loss: 0.0159 - acc: 0.9951 - val_loss: 1.2822 - val_acc: 0.8016\n",
      "Epoch 14/20\n",
      "8000/8000 [==============================] - 4327s - loss: 0.0153 - acc: 0.9953 - val_loss: 1.3655 - val_acc: 0.8098\n",
      "Epoch 15/20\n",
      "8000/8000 [==============================] - 4819s - loss: 0.0143 - acc: 0.9956 - val_loss: 1.4025 - val_acc: 0.7909\n",
      "Epoch 16/20\n",
      "8000/8000 [==============================] - 5808s - loss: 0.0139 - acc: 0.9957 - val_loss: 1.3793 - val_acc: 0.8074\n",
      "Epoch 17/20\n",
      "8000/8000 [==============================] - 5842s - loss: 0.0137 - acc: 0.9960 - val_loss: 1.5173 - val_acc: 0.8006\n",
      "Epoch 18/20\n",
      "8000/8000 [==============================] - 5682s - loss: 0.0133 - acc: 0.9961 - val_loss: 1.4909 - val_acc: 0.8095\n",
      "Epoch 19/20\n",
      "8000/8000 [==============================] - 5870s - loss: 0.0131 - acc: 0.9963 - val_loss: 1.4438 - val_acc: 0.8087\n",
      "Epoch 20/20\n",
      "8000/8000 [==============================] - 5756s - loss: 0.0142 - acc: 0.9961 - val_loss: 1.4633 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d241610>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train our classifier with the training_set\n",
    "# steps_per_epoch is the number of images in training_set\n",
    "# epochs are the number of cycles to repeat the training\n",
    "# validation data is the test_set\n",
    "# validation step corresponds to the 2000 test images\n",
    "classifier.fit_generator(training_set, steps_per_epoch = 8000, epochs = 20, validation_data = test_set, validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 20 epochs, our model was extremely accurate at classifying dogs vs. cats for the training_set - **99.61%**, and our model performed pretty well on the test_set - **81.75%**. To further improve our model, we can always consider adding additional *Convolutional Layers* and/or additional *Fully Connected Layers*. The downside of adding additional convolutional layers is that it adds model complexity and the model will take longer to train. We can also add more training images so our model will be able to extract additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Predictions with CNNs\n",
    "Now we will test the trained CNN model on two images (one of a dog and one of a cat) it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our images\n",
    "test_image1 = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64,64))\n",
    "test_image2 = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Image 1**\n",
    "![Stella](dataset/single_prediction/cat_or_dog_1.jpg)\n",
    "\n",
    "**Test Image 2**\n",
    "![Hobie](dataset/single_prediction/cat_or_dog_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image1 = image.img_to_array(test_image1)\n",
    "test_image2 = image.img_to_array(test_image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image1 = np.expand_dims(test_image1, axis=0)\n",
    "test_image2 = np.expand_dims(test_image2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = classifier.predict(test_image1)\n",
    "result2 = classifier.predict(test_image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if result1[0][0] == 1:\n",
    "    prediction1 = 'dog'\n",
    "else:\n",
    "    prediction1 = 'cat'\n",
    "        \n",
    "if result2[0][0] == 1:\n",
    "    prediction2 = 'dog'\n",
    "else:\n",
    "    prediction2 = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Image 1 Prediction:  dog\n"
     ]
    }
   ],
   "source": [
    "print \"Test Image 1 Prediction: \", prediction1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Image 2 Prediction:  cat\n"
     ]
    }
   ],
   "source": [
    "print \"Test Image 2 Prediction: \", prediction2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
